{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsqGUlzwFW0K",
        "outputId": "3138f081-3e9b-4eb7-ad9c-bfc95f91ead3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, Dropout\n",
        "import tensorflow_datasets as tfds\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Vfu6KMHDDaII",
        "outputId": "e78fd659-6da7-4f64-c9d8-8ffe895b8de3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "\n",
        "news = fetch_20newsgroups(subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
        "news_df = pd.DataFrame({\"text\": news.data, \"category\": news.target})\n",
        "news_df['category_name'] = news_df['category'].map(lambda x: news.target_names[x])\n",
        "news_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySJO8yWzFhrj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df= news_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecu2qBd1GFAu"
      },
      "source": [
        "# **VADER  score**:\n",
        "VADER uses a sentiment lexicon with words annotated with a sentiment score ranging from -4 to 4,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYkMOSAaDfm0"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Add word count and character count\n",
        "train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\n",
        "train_df['char_count'] = train_df['text'].apply(len)\n",
        "\n",
        "# Calculate VADER sentiment scores\n",
        "train_df['vader_scores'] =train_df['text'].apply(\n",
        "    lambda x: sia.polarity_scores(x)['compound']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1GXluVkGI1O"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "- Removes generic words (e.g., would, could, well, also, much, many, even, still, always, get, take, thing).\n",
        "- Lemmatizes words to unify different forms (running, runs, ran â†’ run).\n",
        "- Removes non-essential POS tags like adverbs (RB), conjunctions (CC), determiners (DT), and auxiliary verbs (MD).\n",
        "- Keeps important nouns and verbs to preserve meaning.\n",
        "- Cleans text without over-filtering punctuation (keeps spaces, removes unwanted symbols)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5E-jtLA8IXJS"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3igHn0ZDfpL",
        "outputId": "41645a67-988a-4326-8074-5fd1f5c8ed8d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Expanded stopword list\n",
        "custom_stopwords = set(stopwords.words(\"english\")) | {\n",
        "    \"would\", \"could\", \"like\", \"well\", \"also\", \"way\", \"may\",\n",
        "    \"much\", \"many\", \"even\", \"still\", \"always\", \"get\", \"take\", \"thing\",'ax','one','good'\n",
        "}\n",
        "\n",
        "# POS tags to remove\n",
        "excluded_pos_tags = {\"RB\", \"DT\", \"IN\", \"CC\", \"MD\", \"PRP\", \"WP\", \"EX\"}\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Convert POS tag to WordNet format for lemmatization.\"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing HTML, URLs, and unwanted symbols.\"\"\"\n",
        "    text = re.sub(\"<[^>]*>\", \"\", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)  # Remove URLs\n",
        "    text = unicodedata.normalize(\"NFKD\", text)  # Normalize special characters\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove non-word characters but keep spaces\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Full preprocessing pipeline for text cleaning and filtering.\"\"\"\n",
        "    text = clean_text(text)  # Initial text cleaning\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    words = word_tokenize(text)  # Tokenize text\n",
        "    tagged_words = pos_tag(words)  # POS tagging\n",
        "\n",
        "    filtered_words = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        for word, tag in tagged_words\n",
        "        if word not in custom_stopwords and  # Remove stopwords\n",
        "           len(word) > 1 and  # Remove single characters\n",
        "           not word.isdigit() and  # Remove numbers\n",
        "           tag not in excluded_pos_tags  # Remove unnecessary POS\n",
        "    ]\n",
        "\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "train_df[\"cleaned_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "train_df[\"word_count\"] = train_df[\"cleaned_text\"].str.split().str.len()\n",
        "\n",
        "print(\"Number of empty sentences:\", (train_df[\"word_count\"] == 0).sum())\n",
        "print(\"Average words per sentence:\", train_df[\"word_count\"].mean())\n",
        "print(\"\\nSample of very short cleaned texts:\")\n",
        "print(train_df[train_df[\"word_count\"] < 3][[\"text\", \"cleaned_text\"]].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNhybdKZ3ZGc",
        "outputId": "b7eae92e-8fe3-47b4-9d01-0f5e693cf64b"
      },
      "outputs": [],
      "source": [
        "# Remove rows where cleaned_text is empty or just whitespace\n",
        "train_df = train_df[train_df['cleaned_text'].str.strip() != '']\n",
        "train_df[train_df[\"cleaned_text\"]=='']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "nGBmkUWLGX-3",
        "outputId": "62e6d397-5591-45c4-919b-864101050745"
      },
      "outputs": [],
      "source": [
        "train_df[['text','cleaned_text']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PpnaAE7GefW"
      },
      "source": [
        "# **Perform EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-xnJsOnDfuR"
      },
      "outputs": [],
      "source": [
        "def create_length_distribution_plot():\n",
        "    \"\"\"Create interactive distribution plot for review lengths\"\"\"\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Histogram(\n",
        "        x=train_df['word_count'],\n",
        "        name='Word Count Distribution',\n",
        "        nbinsx=100\n",
        "    ))\n",
        "    fig.update_layout(\n",
        "        title='Distribution of Review Lengths',\n",
        "        xaxis_title='Word Count',\n",
        "        yaxis_title='Frequency'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def create_sentiment_distribution_plot():\n",
        "    \"\"\"Create interactive sentiment distribution plot\"\"\"\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Violin(\n",
        "        y=train_df['vader_scores'],\n",
        "        box_visible=True,\n",
        "        line_color='black',\n",
        "        name='Sentiment Distribution'\n",
        "    ))\n",
        "    fig.update_layout(\n",
        "        title='Distribution of Sentiment Scores',\n",
        "        yaxis_title='VADER Sentiment Score'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def create_wordcloud():\n",
        "    \"\"\"Generate and display word cloud\"\"\"\n",
        "    all_words = ' '.join(train_df['cleaned_text'])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Reviews')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6h9l1yuODfw3",
        "outputId": "88678ef8-0605-4150-9e97-b87330b57532"
      },
      "outputs": [],
      "source": [
        "create_length_distribution_plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fN_bHsH6zVsv",
        "outputId": "1841ba44-4f2e-4bdd-a9a5-475e1266ff55"
      },
      "outputs": [],
      "source": [
        "create_sentiment_distribution_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "tL1QcGpGzViu",
        "outputId": "194a6783-67e2-4bd0-b717-c5f68488b9d3"
      },
      "outputs": [],
      "source": [
        "create_wordcloud()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ufmG12pODfzO",
        "outputId": "aa3c1a9d-e68a-479c-db57-c6ff654b34d4"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.histogram(train_df, x=\"category_name\", nbins=20, title=\"Category Distribution\",\n",
        "                   labels={\"category\": \"category_name\"}, opacity=0.7)\n",
        "fig.update_layout(bargap=0.2)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkDKl9P7JGcf"
      },
      "source": [
        "# **TF-IDF**\n",
        "- The TF-IDF algorithm measures two factors: the frequency of a word in a document (TF) and the frequency of the word across all documents in the corpus (IDF). The term frequency (TF) is a measure of how frequently a term appears in a document.\n",
        "- (TFIDF) is a statistical formula to convert text documents into vectors based on the relevancy of the word. It is based on the bag of the words model to create a matrix containing the information about less relevant and most relevant words in the document.\n",
        "\n",
        "# **KMeans clustering**\n",
        "- K-means groups together similar data points. The aim of the k-means algorithm is to minimize the total distances between points and their assigned cluster centroid.\n",
        "- Type of unsupervised learning, which is used for unlabeled data.\n",
        "- Simple, fast algorithm that is generally unaffected by extremes and outliers.\n",
        "\n",
        "\n",
        "- Both ARI (Adjusted Rand Index) and NMI (Normalized Mutual Information) measure clustering quality by comparing clusters with true labels.\n",
        "\n",
        "\n",
        "```\n",
        "Metric\t   Range\t        Good Score\n",
        "ARI     \t-1 to 1\t       > 0.5 (Strong agreement)\n",
        "NMI\t      0 to 1\t       > 0.6 (Good match)\n",
        "\n",
        "ARI â‰ˆ 0 means clusters are random.\n",
        "ARI > 0.5 indicates clusters align well with labels.\n",
        "NMI > 0.6 suggests good cluster separation.\n",
        "```\n",
        "- For unsupervised sentiment analysis, we usually aim for:\n",
        "  - ARI â‰¥ 0.3 (decent for text data)\n",
        "  - NMI â‰¥ 0.5 (clusters contain meaningful info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "RItGzHIfDf1p",
        "outputId": "2743ee18-a5e0-4d93-db4a-1267dbf2276f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=2000)\n",
        "tfidf_matrix = tfidf.fit_transform(train_df[\"cleaned_text\"])\n",
        "\n",
        "inertia = []\n",
        "k_values = range(2, 15)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot inertia vs. k\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, inertia, marker=\"o\", linestyle=\"--\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk5-4lQhJgZF"
      },
      "outputs": [],
      "source": [
        "pip install kneed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh-HoYaYDf4D",
        "outputId": "3e48e9d0-7ab0-4de4-a4a8-fbfb22f4c937"
      },
      "outputs": [],
      "source": [
        "from kneed import KneeLocator\n",
        "\n",
        "knee_locator = KneeLocator(k_values, inertia, curve=\"convex\", direction=\"decreasing\")\n",
        "optimal_k = knee_locator.elbow\n",
        "\n",
        "print(f\"Optimal k found: {optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6HFwjHWDf6z"
      },
      "outputs": [],
      "source": [
        "optimal_k = 7\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "train_df[\"knn_cluster\"] = kmeans.fit_predict(tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "5gZUH8cKDf9H",
        "outputId": "ba6c4a95-cf49-4da4-e584-4fb08d2fd656"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "fig = px.scatter(\n",
        "    x=reduced_features[:, 0],\n",
        "    y=reduced_features[:, 1],\n",
        "    color=train_df[\"knn_cluster\"].astype(str),\n",
        "    title=\"Review Clusters based on Content Similarity\",\n",
        "    labels={\"color\": \"Cluster\"},\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pe-XxJxDf_T",
        "outputId": "77de3e8c-caf7-4dcd-eaf8-8d64bf09427f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get top words per cluster\n",
        "def get_top_keywords_per_cluster(tfidf_matrix, clusters, feature_names, top_n=10):\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    top_words = {}\n",
        "\n",
        "    for cluster_idx in range(optimal_k):\n",
        "        top_word_indices = np.argsort(cluster_centers[cluster_idx])[-top_n:]\n",
        "        top_words[cluster_idx] = [feature_names[i] for i in top_word_indices]\n",
        "\n",
        "    return top_words\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "top_words = get_top_keywords_per_cluster(tfidf_matrix, train_df[\"knn_cluster\"], feature_names)\n",
        "\n",
        "\n",
        "for cluster, words in top_words.items():\n",
        "    print(f\"\\nðŸŸ¢ Cluster {cluster} Keywords: {', '.join(words)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "5BQOgFP97RUF",
        "outputId": "872e28a8-7e5f-48dc-860a-98d19fd30a52"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "cluster_category_ct = pd.crosstab(train_df['category_name'], train_df['knn_cluster'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(cluster_category_ct, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
        "plt.title(\"Relationship between Category and KNN Cluster\")\n",
        "plt.xlabel(\"KNN Cluster\")\n",
        "plt.ylabel(\"Category Name\")\n",
        "plt.xticks(rotation=0, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOluNivd6YOw"
      },
      "source": [
        "# **Observations:**\n",
        "- Cluster 0 appears to group together all the harware and electronics related articles.\n",
        "- Cluster 1 is dominated by politics and religion topics.\n",
        "- Cluster 2 is all about sports.\n",
        "- Cluster 3 contains mostly religion related news, overlapping with cluster 1.\n",
        "- Clusters 4,5 and 6 have mixed topics.\n",
        "- The clusters are not evenly populated. Most news lie in cluster 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_MvMK_RJ8v5"
      },
      "source": [
        "# **DBSCAN clustering with BERT embeddings**\n",
        "### **DBSCAN Clustering**  \n",
        "DBSCAN (**Density-Based Spatial Clustering of Applications with Noise**) is a **density-based clustering algorithm** that groups data points based on their density.  \n",
        "\n",
        "- **Finds dense regions** and groups them into clusters.  \n",
        "- **Does not require specifying the number of clusters** (unlike KMeans).  \n",
        "- **Can detect noise (outliers)** instead of forcing all points into clusters.  \n",
        "- Uses two key parameters:  \n",
        "  - **Îµ (epsilon)** â†’ Defines how close points must be to be considered neighbors.  \n",
        "  - **min_samples** â†’ Minimum points needed to form a dense region (cluster).  \n",
        "- Works **well for high-dimensional embeddings** (e.g., BERT).  \n",
        "- Can **discover natural groupings** (topics) instead of forcing a fixed number.  \n",
        "- Can **handle noise** (irrelevant or vague reviews).  \n",
        "\n",
        "---\n",
        "\n",
        "### **BERT Embeddings**  \n",
        "BERT (**Bidirectional Encoder Representations from Transformers**) is a deep learning model that creates **contextual word embeddings**.  \n",
        "- They are **vector representations of text**, capturing **meaning** and **context**.  \n",
        "- Unlike TF-IDF, **BERT understands synonyms & context** (e.g., \"bank\" as a financial institution vs. a riverbank).  \n",
        "- Converts reviews into **dense numerical vectors**, making clustering more meaningful.  \n",
        "- **Understands context** better than simple word frequency methods (TF-IDF).  \n",
        "- Works well with **DBSCAN** since it finds natural clusters in high-dimensional space.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use DBSCAN + BERT Here?**  \n",
        "1. **We donâ€™t know the exact number of clusters** â†’ DBSCAN finds natural clusters without predefining them.  \n",
        "2. **Reviews may contain noise** â†’ DBSCAN can ignore irrelevant reviews instead of forcing them into a category.  \n",
        "3. **Sentiment-based clustering is unclear** â†’ BERT embeddings help capture review **semantics**, making clustering more meaningful.  \n",
        "4. **We want a more flexible model** â†’ Unlike KMeans, DBSCAN doesnâ€™t assume clusters are spherical (it handles non-uniform shapes).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wdfeNSHiJ1NX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers scikit-learn scipy nltk tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f409f7caf9494d1aa7375dd4fee8defb",
            "436b50c690ca46079bbfd9ff07032afe",
            "35388947d3d145338ef385bd3ec5098c",
            "ca11803e34204268a7a8808d2b7854f8",
            "70ed72f9d61649799aaa55377d99352b",
            "26e831ba5aee4c5a8943f14c4b03f208",
            "95c741e86f874399bddf370a5006eac9",
            "4bbb12c5c9de4fb2900930476b86cef7",
            "cd7ca1ba3dc24ffaae18ab6b4b5b3152",
            "071f23a8ed114041b6dcabf7309d52c2",
            "e4a829b05eac4821b28512098c0db0c9"
          ]
        },
        "id": "cPL97oUpJ1J2",
        "outputId": "d914a39b-6876-4b51-cc2f-bbaf70d78ee4"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "train_sentences = train_df[\"cleaned_text\"].tolist()\n",
        "train_embeddings = model.encode(train_sentences, convert_to_numpy=True, batch_size=32, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aTxQs8TTJ1HU",
        "outputId": "7ecf72f9-72f2-4e4a-dbea-d3e555c5a699"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "from kneed import KneeLocator\n",
        "\n",
        "def find_optimal_epsilon(data, min_samples, n_neighbors=5):\n",
        "    \"\"\"\n",
        "    Find optimal epsilon using k-nearest neighbors distance plot\n",
        "\n",
        "    Args:\n",
        "        data: feature matrix\n",
        "        min_samples: min_samples parameter for DBSCAN\n",
        "        n_neighbors: number of neighbors to consider (default: 5)\n",
        "\n",
        "    Returns:\n",
        "        optimal epsilon value\n",
        "    \"\"\"\n",
        "    # Calculate distances to nearest neighbors\n",
        "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "    neigh.fit(data)\n",
        "    distances, _ = neigh.kneighbors(data)\n",
        "\n",
        "    # Sort distances in ascending order\n",
        "    distances = np.sort(distances[:, -1])\n",
        "\n",
        "    # Plot k-distance graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(distances)), distances)\n",
        "    plt.xlabel('Points sorted by distance')\n",
        "    plt.ylabel(f'Distance to {n_neighbors}th nearest neighbor')\n",
        "    plt.title('K-distance Graph')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Find elbow point using KneeLocator\n",
        "    kneedle = KneeLocator(\n",
        "        range(len(distances)),\n",
        "        distances,\n",
        "        S=1.0,\n",
        "        curve='convex',\n",
        "        direction='increasing'\n",
        "    )\n",
        "\n",
        "    optimal_epsilon = distances[kneedle.knee]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(distances)), distances)\n",
        "    plt.axhline(y=optimal_epsilon, color='r', linestyle='--',\n",
        "                label=f'Optimal epsilon: {optimal_epsilon:.2f}')\n",
        "    plt.plot(kneedle.knee, distances[kneedle.knee], 'ro')\n",
        "    plt.xlabel('Points sorted by distance')\n",
        "    plt.ylabel(f'Distance to {n_neighbors}th nearest neighbor')\n",
        "    plt.title('K-distance Graph with Optimal Epsilon')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return optimal_epsilon\n",
        "\n",
        "def perform_dbscan_clustering(data, epsilon, min_samples):\n",
        "    \"\"\"\n",
        "    Perform DBSCAN clustering with given parameters\n",
        "\n",
        "    Args:\n",
        "        data: feature matrix\n",
        "        epsilon: epsilon parameter for DBSCAN\n",
        "        min_samples: min_samples parameter for DBSCAN\n",
        "\n",
        "    Returns:\n",
        "        cluster labels and DBSCAN model\n",
        "    \"\"\"\n",
        "    dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
        "    clusters = dbscan.fit_predict(data)\n",
        "    return clusters, dbscan\n",
        "\n",
        "def visualize_clusters(data, clusters):\n",
        "    \"\"\"\n",
        "    Create visualizations for clustering results\n",
        "\n",
        "    Args:\n",
        "        data: feature matrix\n",
        "        clusters: cluster labels\n",
        "    \"\"\"\n",
        "    # Reduce dimensionality for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_features = pca.fit_transform(data)\n",
        "\n",
        "    # Create DataFrame for plotting\n",
        "    df_plot = pd.DataFrame({\n",
        "        'x': reduced_features[:, 0],\n",
        "        'y': reduced_features[:, 1],\n",
        "        'cluster': clusters\n",
        "    })\n",
        "\n",
        "    # Interactive scatter plot with Plotly\n",
        "    fig = px.scatter(\n",
        "        df_plot,\n",
        "        x='x',\n",
        "        y='y',\n",
        "        color='cluster',\n",
        "        title='DBSCAN Clustering Results (PCA)',\n",
        "        labels={'x': 'First Principal Component',\n",
        "                'y': 'Second Principal Component'},\n",
        "        color_continuous_scale='viridis'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    # Plot cluster distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
        "    sns.barplot(x=cluster_counts.index, y=cluster_counts.values)\n",
        "    plt.title('Distribution of Clusters')\n",
        "    plt.xlabel('Cluster')\n",
        "    plt.ylabel('Number of Points')\n",
        "    plt.show()\n",
        "\n",
        "def analyze_clusters(data, clusters):\n",
        "\n",
        "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "    n_noise = list(clusters).count(-1)\n",
        "\n",
        "    metrics = {\n",
        "        'n_clusters': n_clusters,\n",
        "        'n_noise_points': n_noise,\n",
        "        'noise_percentage': n_noise / len(clusters) * 100,\n",
        "        'cluster_sizes': pd.Series(clusters).value_counts().sort_index().to_dict()\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Main clustering pipeline\n",
        "def run_dbscan_analysis(data, min_samples=5):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Find optimal epsilon\n",
        "    print(\"Finding optimal epsilon...\")\n",
        "    epsilon = find_optimal_epsilon(scaled_data, min_samples)\n",
        "    print(f\"Optimal epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    # Perform clustering\n",
        "    print(\"\\nPerforming DBSCAN clustering...\")\n",
        "    clusters, dbscan = perform_dbscan_clustering(scaled_data, epsilon, min_samples)\n",
        "\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    visualize_clusters(scaled_data, clusters)\n",
        "\n",
        "    print(\"\\nAnalyzing clustering results...\")\n",
        "    metrics = analyze_clusters(scaled_data, clusters)\n",
        "\n",
        "    print(\"\\nClustering Results:\")\n",
        "    print(f\"Number of clusters: {metrics['n_clusters']}\")\n",
        "    print(f\"Number of noise points: {metrics['n_noise_points']}\")\n",
        "    print(f\"Percentage of noise points: {metrics['noise_percentage']:.2f}%\")\n",
        "    print(\"\\nCluster sizes:\")\n",
        "    for cluster, size in metrics['cluster_sizes'].items():\n",
        "        print(f\"Cluster {cluster}: {size} points\")\n",
        "\n",
        "    return clusters, metrics\n",
        "\n",
        "clusters, metrics = run_dbscan_analysis(train_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WuoSWl-wMt"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.02\n",
        "min_samples = 10\n",
        "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
        "train_df['DBSCAN_clusters'] = dbscan.fit_predict(train_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fSIsPMHhJ1Ep",
        "outputId": "419846a6-45e8-48a6-e30a-34b0abf08b37"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(train_embeddings)\n",
        "\n",
        "fig = px.scatter(\n",
        "    x=reduced_embeddings[:, 0],\n",
        "    y=reduced_embeddings[:, 1],\n",
        "    color=train_df[\"DBSCAN_clusters\"].astype(str),\n",
        "    title=\"DBSCAN Clustering of IMDb Reviews\",\n",
        "    labels={\"color\": \"Cluster\"}\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEqL8dzVC6P1"
      },
      "source": [
        "# **Hierarchical clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "SDKBWtlZUCdo",
        "outputId": "81e4407a-5411-44fd-9cf5-1cae88b83d7e"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "linkage_matrix = linkage(train_embeddings, method=\"ward\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage_matrix, truncate_mode=\"level\", p=10)\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xlabel(\"News\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZoSSvCiDCES",
        "outputId": "14ce8227-d894-42d9-a694-f3966ae29fce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "import matplotlib.pyplot as plt\n",
        "from kneed import KneeLocator\n",
        "from sklearn.metrics import silhouette_score\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def find_optimal_clusters(embeddings, max_clusters=20):\n",
        "    \"\"\"\n",
        "    Find optimal number of clusters using multiple methods:\n",
        "    1. Elbow method using distortion\n",
        "    2. Silhouette analysis\n",
        "    3. Dendrogram analysis\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate distortion scores (within-cluster sum of squares)\n",
        "    distortions = []\n",
        "    silhouette_scores = []\n",
        "    n_clusters_range = range(2, max_clusters + 1)\n",
        "\n",
        "    for n_clusters in n_clusters_range:\n",
        "        cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
        "\n",
        "        # Calculate distortion\n",
        "        distortion = 0\n",
        "        for i in range(1, n_clusters + 1):\n",
        "            cluster_points = embeddings[cluster_labels == i]\n",
        "            centroid = np.mean(cluster_points, axis=0)\n",
        "            distortion += np.sum((cluster_points - centroid) ** 2)\n",
        "        distortions.append(distortion)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        if len(np.unique(cluster_labels)) > 1:  # Silhouette requires at least 2 clusters\n",
        "            silhouette_scores.append(silhouette_score(embeddings, cluster_labels))\n",
        "        else:\n",
        "            silhouette_scores.append(0)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=list(n_clusters_range),\n",
        "                            y=distortions,\n",
        "                            mode='lines+markers',\n",
        "                            name='Distortion'))\n",
        "\n",
        "    kn = KneeLocator(list(n_clusters_range),\n",
        "                     distortions,\n",
        "                     curve='convex',\n",
        "                     direction='decreasing')\n",
        "    elbow_point = kn.knee\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=[elbow_point],\n",
        "                            y=[distortions[elbow_point-2]],\n",
        "                            mode='markers',\n",
        "                            marker=dict(size=15, color='red'),\n",
        "                            name=f'Elbow Point (k={elbow_point})'))\n",
        "\n",
        "    fig.update_layout(title='Elbow Method for Optimal k',\n",
        "                     xaxis_title='Number of Clusters (k)',\n",
        "                     yaxis_title='Distortion Score',\n",
        "                     showlegend=True)\n",
        "    fig.show()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=list(n_clusters_range),\n",
        "                            y=silhouette_scores,\n",
        "                            mode='lines+markers',\n",
        "                            name='Silhouette Score'))\n",
        "\n",
        "    # Find optimal k using silhouette score\n",
        "    optimal_k_silhouette = n_clusters_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=[optimal_k_silhouette],\n",
        "                            y=[max(silhouette_scores)],\n",
        "                            mode='markers',\n",
        "                            marker=dict(size=15, color='red'),\n",
        "                            name=f'Optimal k={optimal_k_silhouette}'))\n",
        "\n",
        "    fig.update_layout(title='Silhouette Scores for Different k',\n",
        "                     xaxis_title='Number of Clusters (k)',\n",
        "                     yaxis_title='Silhouette Score',\n",
        "                     showlegend=True)\n",
        "    fig.show()\n",
        "\n",
        "    # Calculate inconsistency coefficients\n",
        "    def get_inconsistency(linkage_matrix):\n",
        "        n = len(linkage_matrix) + 1\n",
        "        heights = linkage_matrix[:, 2]\n",
        "        mean_heights = np.mean(heights)\n",
        "        std_heights = np.std(heights)\n",
        "        inconsistency = (heights - mean_heights) / std_heights if std_heights > 0 else heights\n",
        "        return inconsistency\n",
        "\n",
        "    inconsistency = get_inconsistency(linkage_matrix)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(range(len(inconsistency)), sorted(inconsistency, reverse=True))\n",
        "    plt.title('Sorted Inconsistency Coefficients')\n",
        "    plt.xlabel('Merge Index')\n",
        "    plt.ylabel('Inconsistency Coefficient')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nOptimal Cluster Analysis Results:\")\n",
        "    print(f\"1. Elbow Method suggests {elbow_point} clusters\")\n",
        "    print(f\"2. Silhouette Analysis suggests {optimal_k_silhouette} clusters\")\n",
        "    print(\"3. Dendrogram Analysis: Check the dendrogram plot for major splits\")\n",
        "\n",
        "    return {\n",
        "        'elbow_k': elbow_point,\n",
        "        'silhouette_k': optimal_k_silhouette,\n",
        "        'distortions': distortions,\n",
        "        'silhouette_scores': silhouette_scores,\n",
        "        'linkage_matrix': linkage_matrix\n",
        "    }\n",
        "\n",
        "def apply_clustering(embeddings, n_clusters, linkage_matrix=None):\n",
        "    \"\"\"\n",
        "    Apply hierarchical clustering with the optimal number of clusters\n",
        "    \"\"\"\n",
        "    if linkage_matrix is None:\n",
        "        linkage_matrix = linkage(embeddings, method=\"ward\")\n",
        "\n",
        "    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
        "    cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "\n",
        "    print(\"\\nCluster Statistics:\")\n",
        "    print(f\"Number of clusters: {n_clusters}\")\n",
        "    print(\"\\nCluster sizes:\")\n",
        "    for cluster, size in cluster_sizes.items():\n",
        "        print(f\"Cluster {cluster}: {size} samples\")\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "results = find_optimal_clusters(train_embeddings, max_clusters=20)\n",
        "n_clusters = results['elbow_k']  # or results['silhouette_k']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nd_yozQBNOl"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_clusters = 8\n",
        "clusters = fcluster(linkage_matrix, num_clusters, criterion=\"maxclust\")\n",
        "\n",
        "train_df[\"h_cluster\"] = clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6xlQ4WbxDJn2",
        "outputId": "20008fab-af20-42f8-f5a0-b080518a5347"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(train_embeddings)\n",
        "\n",
        "fig = px.scatter(\n",
        "    x=reduced_embeddings[:, 0],\n",
        "    y=reduced_embeddings[:, 1],\n",
        "    color=train_df[\"h_cluster\"].astype(str),\n",
        "    title=\"Hierarchical clustering\",\n",
        "    labels={\"color\": \"Cluster\"}\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "pKl6PK5pE8Oz",
        "outputId": "aecfdc41-0241-4084-df26-53ccd9d59a2c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "cluster_category_ct = pd.crosstab(train_df['category_name'], train_df['h_cluster'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(cluster_category_ct, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
        "plt.title(\"Relationship between Category and Hierarchical Cluster\")\n",
        "plt.xlabel(\"Hierarchical Cluster\")\n",
        "plt.ylabel(\"Category Name\")\n",
        "plt.xticks(rotation=0, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAzCah_9-iiP"
      },
      "source": [
        "# **Observations:**\n",
        "- Cluster 1 appears to group together all the harware and electronics related articles.\n",
        "- Cluster 2 is dominated by the space field.\n",
        "- Cluster 3 overlaps with cluster 1. However, it is dominated by windows and graphics news while cluster 1 concentrates more on hardware.\n",
        "- Cluster 4 contains mostly politics related news.\n",
        "- Cluster 5 has articles about religion and very few on politics.\n",
        "- Cluster 6 is all about sports.\n",
        "- Cluster 7 contains everything on vehicles, motocyles and cars.\n",
        "- Cluster 8 is a mix of everything, with a great focus on medicine and politics again. This appears to be the largest cluster.\n",
        "- The clusters are well populated and separetd between them. Some overlap is expected since many of the topics naturally are linked between them.\n",
        "- The clustering here clearly is better compared to KNN method."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "071f23a8ed114041b6dcabf7309d52c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e831ba5aee4c5a8943f14c4b03f208": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35388947d3d145338ef385bd3ec5098c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bbb12c5c9de4fb2900930476b86cef7",
            "max": 572,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd7ca1ba3dc24ffaae18ab6b4b5b3152",
            "value": 572
          }
        },
        "436b50c690ca46079bbfd9ff07032afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26e831ba5aee4c5a8943f14c4b03f208",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95c741e86f874399bddf370a5006eac9",
            "value": "Batches:â€‡100%"
          }
        },
        "4bbb12c5c9de4fb2900930476b86cef7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ed72f9d61649799aaa55377d99352b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c741e86f874399bddf370a5006eac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca11803e34204268a7a8808d2b7854f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071f23a8ed114041b6dcabf7309d52c2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e4a829b05eac4821b28512098c0db0c9",
            "value": "â€‡572/572â€‡[13:24&lt;00:00,â€‡â€‡6.14it/s]"
          }
        },
        "cd7ca1ba3dc24ffaae18ab6b4b5b3152": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4a829b05eac4821b28512098c0db0c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f409f7caf9494d1aa7375dd4fee8defb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_436b50c690ca46079bbfd9ff07032afe",
              "IPY_MODEL_35388947d3d145338ef385bd3ec5098c",
              "IPY_MODEL_ca11803e34204268a7a8808d2b7854f8"
            ],
            "layout": "IPY_MODEL_70ed72f9d61649799aaa55377d99352b"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
